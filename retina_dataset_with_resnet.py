# -*- coding: utf-8 -*-
"""Retina Dataset with ResNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OnJa2v5fwHni8l4atgZM_zBwSEKFxSeI

**Step 1: Load the dataset directory from kaggle Using Api**
"""

import os
os.environ['KAGGLE_USERNAME'] = "latasaha" # username from the json file
os.environ['KAGGLE_KEY'] = "39622628149705a697b5ab6d9f7f4d6a" # key from the json file

!kaggle datasets download -d tanlikesmath/diabetic-retinopathy-resized

# Unzip training data
from zipfile import ZipFile
file_name = "/content/diabetic-retinopathy-resized.zip"
with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('done')

import numpy as np 
import pandas as pd

data = pd.read_csv("/content/trainLabels.csv")
data.head()

data.head()

data['image_name'] = [i+".jpeg" for i in data['image'].values]
data.head()

data['level'].hist()
data['level'].value_counts()

from sklearn.model_selection import train_test_split

train, test = train_test_split(data, test_size=0.20, shuffle=True, random_state=42)
train, val = train_test_split(train, test_size=0.15, shuffle=True, random_state=42)

train.shape, test.shape, val.shape

from keras.preprocessing.image import ImageDataGenerator

import cv2
def load_ben_color(image):
    IMG_SIZE = 224
    sigmaX=10
    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)
    return image

data_gen = ImageDataGenerator(rescale=1/255.,
                              zoom_range=0.15,
                              fill_mode='constant',
                              cval=0.,
                              horizontal_flip=True,
                              vertical_flip=True,
                              preprocessing_function=load_ben_color)

# batch size
bs = 128

train_gen = data_gen.flow_from_dataframe(train, 
                                         "/content/resized_train/resized_train",
                                         x_col="image_name", y_col="level", class_mode="raw",
                                         batch_size=bs,
                                         target_size=(224, 224))
test_gen = data_gen.flow_from_dataframe(test,
                                       "/content/resized_train/resized_train",
                                       x_col="image_name", y_col="level", class_mode="raw",
                                       batch_size=bs,
                                       target_size=(224, 224))
val_gen = data_gen.flow_from_dataframe(val,
                                       "/content/resized_train/resized_train",
                                       x_col="image_name", y_col="level", class_mode="raw",
                                       batch_size=bs,
                                       target_size=(224, 224))

import tensorflow
from keras.applications.densenet import DenseNet121
from keras.applications.resnet_v2 import ResNet50V2
import keras.layers as L
from keras.models import Model

base_model = ResNet50V2(weights='imagenet',
                   include_top=False,
                   input_shape=(224, 224, 3),)
base_model.trainable = False

model = tensorflow.keras.models.Sequential()
model.add(base_model)
model.add(L.Conv2D(64, (3,3), activation='relu', padding='same'))
model.add(L.MaxPooling2D((3,3), strides=(3,3)))
model.add(L.Dropout(0.2))
model.add(L.BatchNormalization())
model.add(L.Conv2D(32, (2,2), activation='relu', padding='same'))
model.add(L.MaxPooling2D((2,2), strides=(2,2), padding='same'))
model.add(L.Dropout(0.15))
model.add(L.Dense(512, activation='relu'))
model.add(L.Flatten())
model.add(L.Dense(256, activation='relu'))
model.add(L.Dense(64, activation='relu'))
model.add(L.Dropout(0.2))
model.add(L.Dense(16, activation='relu'))
model.add(L.Dense(5, activation='softmax'))

from tensorflow.keras.optimizers import Adam
op = Adam(learning_rate = 0.01, decay = 0.0001)
model.compile(optimizer=op, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='aug_model_plot.png', show_shapes=True, show_layer_names=True)

from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
model_chk = ModelCheckpoint("best_model.h5", save_best_only=True, verbose = 2, monitor="val_accuracy")
# reduce_lr = ReduceLROnPlateau()

history = model.fit(train_gen,
                    epochs=5,
                    verbose=1,
                    validation_data=val_gen,
                    callbacks = [model_chk]
                    )

from keras.models import load_model
model = load_model("best_model.h5")

model.evaluate(test_gen, batch_size = bs, verbose=1)

#Testing the model
from google.colab import files
uploaded = files.upload()

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.image import imread
import pathlib

#Showing image
uploaded_img = plt.imread('10_left.jpeg',0)
img = plt.imshow(uploaded_img)

#Resizing image
from skimage.transform import resize
resized_img = resize(uploaded_img, (224,224,3))
img = plt.imshow(resized_img)

#Getting the models prediction
import numpy as np
pred = model.predict(np.array([resized_img]))

#Showing predictions
pred

#Sorting predictions descending
list_index = [0,1,2,3,4]
x = pred

for i in range(5):
  for j in range(5):
    if x[0][list_index[i]] > x[0][list_index[j]]:
      temp = list_index[i]
      list_index[i] = list_index[j]
      list_index[j] = temp
  
#showing sorted label in order
#print the first 5 most likely classifications
for i in range(5):
  print(list_index[i], ':', pred[0][list_index[i]]*100, '%')